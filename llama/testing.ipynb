{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc52df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "import torch\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b697c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb92734",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = \"meta-llama/Llama-2-7b-hf\"\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model, device_map={\"\": 0}, quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f32e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, \"kaitchup/Llama-2-7b-mt-French-to-English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4164d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text_translate = \"Tu es le seul client du magasin.\"\n",
    "\n",
    "prompt = my_text_translate+\" ###>\"\n",
    "\n",
    "tokenized_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = tokenized_input[\"input_ids\"].cuda()\n",
    "\n",
    "generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        num_beams=6,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=130\n",
    "\n",
    ")\n",
    "for seq in generation_output.sequences:\n",
    "    output = tokenizer.decode(seq, skip_special_tokens=True)\n",
    "    print(output.split(\"###>\")[1].strip())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
